{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-22 19:21:09.670229: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-09-22 19:21:09.746759: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-09-22 19:21:10.177375: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-09-22 19:21:11.257352: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "#pip3 --python=`which python` install .\n",
    "import flowsig as fs\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sc.settings.set_figure_params(dpi=300)\n",
    "sc.settings.figdir = '/data/project/AI4Omic/MASLD/results/scRNA/flowsig'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CCC_control = pd.read_csv('/data/project/AI4Omic/MASLD/results/scRNA/CellphoneDB/statistical_analysis_significant_means_Control.txt', index_col=0, sep='\\t')\n",
    "CCC_control.dropna(subset=['gene_a', 'gene_b'], inplace=True)\n",
    "CCC_control = CCC_control[CCC_control['secreted'] == True]\n",
    "CCC_control = CCC_control[(CCC_control['receptor_a'] == False) & (CCC_control['receptor_b'] == True)]\n",
    "CCC_control.rename(columns={'classification': 'pathway_name', 'annotation_strategy': 'evidence', 'gene_a': 'ligand', 'gene_b': 'receptor'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data and cell-cell communication inference\n",
    "adata = sc.read_h5ad('/data/project/AI4Omic/MASLD/results/scRNA/CellphoneDB/adata_major_29samples.h5ad')\n",
    "condition_key = 'disease_status'\n",
    "\n",
    "# Load the cell-cell communication inference\n",
    "cellchat_ctrl = pd.read_csv('/data/project/AI4Omic/MASLD/results/scRNA/flowsig/cellchat_communications_ctrl.csv', index_col=0)\n",
    "cellchat_masl = pd.read_csv('/data/project/AI4Omic/MASLD/results/scRNA/flowsig/cellchat_communications_masl.csv', index_col=0)\n",
    "cellchat_mash = pd.read_csv('/data/project/AI4Omic/MASLD/results/scRNA/flowsig/cellchat_communications_mash.csv', index_col=0)\n",
    "\n",
    "\n",
    "#adata = adata[adata.obs[condition_key].isin(['CTRL', 'MASL'])].copy()\n",
    "#adata.uns['cellchat_output'] = { 'CTRL': cellchat_ctrl, 'MASL': cellchat_masl}\n",
    "\n",
    "adata = adata[adata.obs[condition_key].isin(['MASH', 'MASL'])].copy()\n",
    "adata.uns['cellchat_output'] = { 'MASL': cellchat_masl, 'MASH': cellchat_mash}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 9376 unannotated or noncoding genes.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "adata = adata[:, ~adata.var_names.str.startswith(('MT-', 'RPL', 'RPS'))].copy()\n",
    "pattern = r\"^(AC|AL|AP)[0-9A-Z\\-]+\\.\\d+$\"\n",
    "to_remove = [g for g in adata.var_names if re.match(pattern, g)]\n",
    "print(f\"Removed {len(to_remove)} unannotated or noncoding genes.\")\n",
    "adata = adata[:, ~adata.var_names.isin(to_remove)].copy()\n",
    "\n",
    "#adata = adata[:, mask].copy()\n",
    "# !pip --python=`which python` install scikit-misc for flavor='seurat_v3'\n",
    "#sc.pp.highly_variable_genes(adata, n_top_genes=20000, inplace=True, batch_key='sample', flavor='seurat_v3')\n",
    "#adata = adata[:, adata.var.highly_variable].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "len(np.unique(adata.obs['sample']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 3954 genes not expressing in MASL.\n",
      "Removing 2743 genes not expressing in MASH.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [29:31<00:00, 59.05s/it]\n"
     ]
    }
   ],
   "source": [
    "# Construct GEMs from the raw count data by the iNMF algorithm by pyliger\n",
    "adata.var.index.name = \"gene\"\n",
    "adata.obs.index.name = \"cell\"\n",
    "fs.pp.construct_gems_using_pyliger(adata, n_gems=30, layer_key='counts', condition_key=condition_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct the flow expression matrices\n",
    "\n",
    "### We construct augmented flow expression matrices for each condition that measure three types of variables:\n",
    "\n",
    "1. Intercellular signal inflow, i.e., how much of a signal did a cell receive. For non-spatial scRNA-seq, signal inflow is defined as receptor gene expression weighted by the average expression of immediate downstream transcription factors that indicate signal activation.\n",
    "2. GEMs, which encapsulate intracellular information processing. We define these as cellwise membership to the GEM.\n",
    "Intercellular signal outflow, i.e., how much of a signal did a cell send. These are simply ligand gene expression.\n",
    "3. The kay assumption of flowsig is that all intercellular information flows are directed from signal inflows to GEMs, from one GEM to another GEM, and from GEMs to signal outflows.\n",
    "\n",
    "For non-spatial scRNA-seq, we need to specify the model organism so that FlowSig knows which receptor-transcription factor targets list to look at."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "help(fs.pp.construct_flows_from_cellphonedb)\n",
    "fs.pp.construct_flows_from_cellphonedb(adata,\n",
    "                                cellphonedb_output_key = ,                                \n",
    "                                gem_expr_key = 'X_gem',\n",
    "                                cellphonedb_tfs_key = '',\n",
    "                                scale_gem_expr = True,\n",
    "                                model_organism = 'human',\n",
    "                                flowsig_network_key = 'flowsig_network',\n",
    "                                flowsig_expr_key = 'X_flow'\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.pp.construct_flows_from_cellchat(adata,\n",
    "                                cellchat_output_key='cellchat_output',                         \n",
    "                                gem_expr_key='X_gem',\n",
    "                                scale_gem_expr = True,\n",
    "                                model_organism = 'human',\n",
    "                                flowsig_network_key = 'flowsig_network',\n",
    "                                flowsig_expr_key = 'X_flow'\n",
    "                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To reduce the number of variables over which we have to infer intercellular flows—and thus computation time—and to prioritise 'informative variables',\n",
    "\n",
    "- we only retain inflow and outflow signals that are sufficiently differentially flowing between the control and perturbed conditions. \n",
    "\n",
    "- We determine differentially flowing signals using a Wilcoxon rank-sum test and retain variables only if they are below a specified adjusted p-value threshold (q-value) and above a specified log-fold-change threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "... storing 'Type' as categorical\n",
      "... storing 'Downstream_TF' as categorical\n",
      "... storing 'Type' as categorical\n",
      "... storing 'Downstream_TF' as categorical\n"
     ]
    }
   ],
   "source": [
    "fs.pp.determine_informative_variables(adata,  \n",
    "                                    flowsig_expr_key = 'X_flow',\n",
    "                                    flowsig_network_key = 'flowsig_network',\n",
    "                                    spatial= False,\n",
    "                                    condition_key = condition_key,\n",
    "                                    control_key = 'MASL',  # CTRL MASL\n",
    "                                    qval_threshold = 0.01,\n",
    "                                    logfc_threshold= 0.8 # 0.8 for MASH versus MASL\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If you want to visualise which variables remained, e.g., which are differentially outflowing, you can run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "... storing 'Type' as categorical\n",
      "... storing 'Downstream_TF' as categorical\n",
      "WARNING:matplotlib.text:posx and posy should be finite values\n",
      "WARNING:matplotlib.text:posx and posy should be finite values\n",
      "WARNING:matplotlib.text:posx and posy should be finite values\n",
      "WARNING:matplotlib.text:posx and posy should be finite values\n"
     ]
    }
   ],
   "source": [
    "from adjustText import adjust_text\n",
    "\n",
    "for disease_status in ['MASH']: # MASH  MASL\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    scatter = fs.pl.plot_differentially_flowing_signals(adata,\n",
    "                                            condition_key = condition_key,\n",
    "                                            pert_key = disease_status,\n",
    "                                            var_type = 'inflow',\n",
    "                                            flowsig_expr_key = 'X_flow_orig',\n",
    "                                            flowsig_network_key = 'flowsig_network_orig',\n",
    "                                            qval_threshold = 0.01,\n",
    "                                            logfc_threshold = 0.8,\n",
    "                                            label_lowqval = True,\n",
    "                                            scatter_size = 60,\n",
    "                                            ax=ax,\n",
    "                                            )\n",
    "    texts = [child for child in ax.get_children() if isinstance(child, plt.Text)]\n",
    "    adjust_text(texts, ax=ax)\n",
    "    plt.tight_layout()\n",
    "    plt.grid(False)\n",
    "    plt.savefig(f'/data/project/AI4Omic/MASLD/results/scRNA/flowsig/flowsig_inflow_{disease_status}.png', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'flow_var_info':                  Type                                      Downstream_TF  \\\n",
       " WNT10A        outflow                                                      \n",
       " EREG          outflow                                                      \n",
       " CXCL1         outflow                                                      \n",
       " CXCL2         outflow                                                      \n",
       " CXCL3         outflow                                                      \n",
       " CXCL6         outflow                                                      \n",
       " CXCL8         outflow                                                      \n",
       " CXCL9         outflow                                                      \n",
       " IL1A          outflow                                                      \n",
       " IL33          outflow                                                      \n",
       " TNFSF15       outflow                                                      \n",
       " SPP1          outflow                                                      \n",
       " ANGPTL1       outflow                                                      \n",
       " C3            outflow                                                      \n",
       " CTSG          outflow                                                      \n",
       " HGF           outflow                                                      \n",
       " CALCRL         inflow                                                      \n",
       " CSF1R          inflow  ABL1_AGAP2_ATF2_BAD_BAX_BCL3_BCLAF1_CDX2_CEBPA...   \n",
       " IL13RA2        inflow  CAT_CEBPA_CTNNB1_DAB2_EP300_ESRRA_FLI1_FOSL1_H...   \n",
       " IL1R2          inflow  ABL1_ANXA1_AR_ATF2_BAD_BRCA1_CD59_CDK2AP1_CEBP...   \n",
       " IL4R+IL13RA1   inflow  AR_BAD_CAT_CEBPA_CTNNB1_DAB2_EP300_ESRRA_FLI1_...   \n",
       " LIFR+IL6ST     inflow  AGAP2_AR_ATF4_BAD_BAX_BRCA1_CREB1_CTNNB1_DIABL...   \n",
       " LRP1           inflow                                       CTNNB1_STAT3   \n",
       " NRP2+PLXNA1    inflow                            HES1_HEY1_HEY2_MYC_RBPJ   \n",
       " GEM-1          module                                                      \n",
       " GEM-2          module                                                      \n",
       " GEM-3          module                                                      \n",
       " GEM-4          module                                                      \n",
       " GEM-5          module                                                      \n",
       " GEM-6          module                                                      \n",
       " GEM-7          module                                                      \n",
       " GEM-8          module                                                      \n",
       " GEM-9          module                                                      \n",
       " GEM-10         module                                                      \n",
       " GEM-11         module                                                      \n",
       " GEM-12         module                                                      \n",
       " GEM-13         module                                                      \n",
       " GEM-14         module                                                      \n",
       " GEM-15         module                                                      \n",
       " GEM-16         module                                                      \n",
       " GEM-17         module                                                      \n",
       " GEM-18         module                                                      \n",
       " GEM-19         module                                                      \n",
       " GEM-20         module                                                      \n",
       " GEM-21         module                                                      \n",
       " GEM-22         module                                                      \n",
       " GEM-23         module                                                      \n",
       " GEM-24         module                                                      \n",
       " GEM-25         module                                                      \n",
       " GEM-26         module                                                      \n",
       " GEM-27         module                                                      \n",
       " GEM-28         module                                                      \n",
       " GEM-29         module                                                      \n",
       " GEM-30         module                                                      \n",
       " \n",
       "                                                     Interaction  \n",
       " WNT10A        WNT10A - (FZD1+LRP5)/WNT10A - (FZD4+LRP5)/WNT1...  \n",
       " EREG          EREG - EGFR/EREG - (EGFR+ERBB2)/EREG - ERBB4/E...  \n",
       " CXCL1                               CXCL1 - CXCR1/CXCL1 - CXCR2  \n",
       " CXCL2                               CXCL2 - CXCR1/CXCL2 - CXCR2  \n",
       " CXCL3                               CXCL3 - CXCR1/CXCL3 - CXCR2  \n",
       " CXCL6                               CXCL6 - CXCR1/CXCL6 - CXCR2  \n",
       " CXCL8                               CXCL8 - CXCR1/CXCL8 - CXCR2  \n",
       " CXCL9                                             CXCL9 - CXCR3  \n",
       " IL1A                         IL1A - (IL1R1+IL1RAP)/IL1A - IL1R2  \n",
       " IL33                                     IL33 - (IL1RL1+IL1RAP)  \n",
       " TNFSF15                                      TNFSF15 - TNFRSF25  \n",
       " SPP1          SPP1 - CD44/SPP1 - (ITGAV+ITGB1)/SPP1 - (ITGAV...  \n",
       " ANGPTL1                  ANGPTL1 - (ITGA1+ITGB1)/ANGPTL1 - PIRB  \n",
       " C3            C3 - C3AR1/C3 - CR2/C3 - (ITGAM+ITGB2)/C3 - (I...  \n",
       " CTSG                        CTSG - F2R/CTSG - PARD3/CTSG - FPR1  \n",
       " HGF                                                   HGF - MET  \n",
       " CALCRL                              ADM - CALCRL/CALCA - CALCRL  \n",
       " CSF1R                                 CSF1 - CSF1R/IL34 - CSF1R  \n",
       " IL13RA2                                          IL13 - IL13RA2  \n",
       " IL1R2                                 IL1A - IL1R2/IL1B - IL1R2  \n",
       " IL4R+IL13RA1         IL13 - (IL4R+IL13RA1)/IL4 - (IL4R+IL13RA1)  \n",
       " LIFR+IL6ST                LIF - (LIFR+IL6ST)/OSM - (LIFR+IL6ST)  \n",
       " LRP1                                                 MDK - LRP1  \n",
       " NRP2+PLXNA1       SEMA3B - (NRP2+PLXNA1)/SEMA3F - (NRP2+PLXNA1)  \n",
       " GEM-1                                                            \n",
       " GEM-2                                                            \n",
       " GEM-3                                                            \n",
       " GEM-4                                                            \n",
       " GEM-5                                                            \n",
       " GEM-6                                                            \n",
       " GEM-7                                                            \n",
       " GEM-8                                                            \n",
       " GEM-9                                                            \n",
       " GEM-10                                                           \n",
       " GEM-11                                                           \n",
       " GEM-12                                                           \n",
       " GEM-13                                                           \n",
       " GEM-14                                                           \n",
       " GEM-15                                                           \n",
       " GEM-16                                                           \n",
       " GEM-17                                                           \n",
       " GEM-18                                                           \n",
       " GEM-19                                                           \n",
       " GEM-20                                                           \n",
       " GEM-21                                                           \n",
       " GEM-22                                                           \n",
       " GEM-23                                                           \n",
       " GEM-24                                                           \n",
       " GEM-25                                                           \n",
       " GEM-26                                                           \n",
       " GEM-27                                                           \n",
       " GEM-28                                                           \n",
       " GEM-29                                                           \n",
       " GEM-30                                                           ,\n",
       " 'network': {'adjacency': array([[0.   , 0.004, 0.   , ..., 0.008, 0.004, 0.01 ],\n",
       "         [0.002, 0.   , 0.946, ..., 0.514, 0.526, 0.502],\n",
       "         [0.002, 0.056, 0.   , ..., 0.03 , 0.064, 0.066],\n",
       "         ...,\n",
       "         [0.018, 0.476, 0.588, ..., 0.   , 0.316, 0.42 ],\n",
       "         [0.008, 0.496, 0.928, ..., 0.326, 0.   , 0.254],\n",
       "         [0.01 , 0.542, 0.934, ..., 0.496, 0.332, 0.   ]]),\n",
       "  'adjacency_validated': array([[0.   , 0.   , 0.   , ..., 0.008, 0.004, 0.01 ],\n",
       "         [0.   , 0.   , 0.   , ..., 0.514, 0.526, 0.502],\n",
       "         [0.   , 0.   , 0.   , ..., 0.03 , 0.064, 0.066],\n",
       "         ...,\n",
       "         [0.018, 0.476, 0.588, ..., 0.   , 0.316, 0.42 ],\n",
       "         [0.008, 0.496, 0.928, ..., 0.326, 0.   , 0.254],\n",
       "         [0.01 , 0.542, 0.934, ..., 0.496, 0.332, 0.   ]]),\n",
       "  'adjacency_validated_filtered': array([[0.   , 0.   , 0.   , ..., 0.   , 0.   , 0.   ],\n",
       "         [0.   , 0.   , 0.   , ..., 0.514, 0.526, 0.502],\n",
       "         [0.   , 0.   , 0.   , ..., 0.   , 0.064, 0.066],\n",
       "         ...,\n",
       "         [0.   , 0.476, 0.   , ..., 0.   , 0.   , 0.42 ],\n",
       "         [0.   , 0.496, 0.928, ..., 0.   , 0.   , 0.   ],\n",
       "         [0.   , 0.542, 0.934, ..., 0.496, 0.   , 0.   ]]),\n",
       "  'flow_vars': array(['WNT10A', 'EREG', 'CXCL1', 'CXCL2', 'CXCL3', 'CXCL6', 'CXCL8',\n",
       "         'CXCL9', 'IL1A', 'IL33', 'TNFSF15', 'SPP1', 'ANGPTL1', 'C3',\n",
       "         'CTSG', 'HGF', 'CALCRL', 'CSF1R', 'IL13RA2', 'IL1R2',\n",
       "         'IL4R+IL13RA1', 'LIFR+IL6ST', 'LRP1', 'NRP2+PLXNA1', 'GEM-1',\n",
       "         'GEM-2', 'GEM-3', 'GEM-4', 'GEM-5', 'GEM-6', 'GEM-7', 'GEM-8',\n",
       "         'GEM-9', 'GEM-10', 'GEM-11', 'GEM-12', 'GEM-13', 'GEM-14',\n",
       "         'GEM-15', 'GEM-16', 'GEM-17', 'GEM-18', 'GEM-19', 'GEM-20',\n",
       "         'GEM-21', 'GEM-22', 'GEM-23', 'GEM-24', 'GEM-25', 'GEM-26',\n",
       "         'GEM-27', 'GEM-28', 'GEM-29', 'GEM-30'], dtype=object),\n",
       "  'perturbed_targets': array([[1.   , 1.   , 1.   , 1.   , 1.   , 1.   , 1.   , 1.   , 1.   ,\n",
       "          1.   , 1.   , 1.   , 1.   , 1.   , 1.   , 1.   , 1.   , 1.   ,\n",
       "          1.   , 1.   , 1.   , 1.   , 1.   , 1.   , 1.   , 0.998, 1.   ,\n",
       "          1.   , 1.   , 1.   , 1.   , 1.   , 1.   , 1.   , 1.   , 1.   ,\n",
       "          1.   , 1.   , 0.99 , 1.   , 1.   , 1.   , 1.   , 1.   , 0.998,\n",
       "          1.   , 1.   , 0.99 , 0.97 , 1.   , 1.   , 1.   , 1.   , 1.   ]])}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.uns['flowsig_network']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "... storing 'Type' as categorical\n",
      "... storing 'Downstream_TF' as categorical\n",
      "WARNING:matplotlib.text:posx and posy should be finite values\n",
      "WARNING:matplotlib.text:posx and posy should be finite values\n",
      "WARNING:matplotlib.text:posx and posy should be finite values\n",
      "WARNING:matplotlib.text:posx and posy should be finite values\n"
     ]
    }
   ],
   "source": [
    "for disease_status in ['MASH']: # MASL, MASH\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    fs.pl.plot_differentially_flowing_signals(adata,\n",
    "                                            condition_key = condition_key,\n",
    "                                            pert_key = disease_status,\n",
    "                                            var_type = 'outflow',\n",
    "                                            flowsig_expr_key = 'X_flow_orig',\n",
    "                                            flowsig_network_key = 'flowsig_network_orig',\n",
    "                                            qval_threshold = 0.01,\n",
    "                                            logfc_threshold = 1.0,\n",
    "                                            label_lowqval = True,\n",
    "                                            ax=ax,\n",
    "                                            scatter_size = 60,\n",
    "                                            )\n",
    "    texts = [child for child in ax.get_children() if isinstance(child, plt.Text)]\n",
    "    adjust_text(texts, ax=ax)\n",
    "    plt.tight_layout()\n",
    "    plt.grid(False)\n",
    "    plt.savefig(f'/data/project/AI4Omic/MASLD/results/scRNA/flowsig/flowsig_outflow_{disease_status}.png', dpi=600, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn intercellular flows\n",
    "We are now in a position to learn the intercellular flows. To increase reliability of objects, we bootstrap aggregate results over a number of realisations. For non-spatial data, we have to specify the condition label and the control condition.\n",
    "\n",
    "This step uses UT-IGSP to learn what is called a completed partially directed acyclic graph (CPDAG), which encodes directed arcs and undirected edges that describe the Markov Equivalence Class of statistical dependence relations that were learned directly from the data using conditional independence testing (how do variables depend on one another) and conditional invariance testing (which variables changed significantly between conditions). For both tests, we use a parametric partial-correlation-based method. The main reason we used these tests were because they take the least time to run compared to nonparametric kernel-based tests. Any test like the Hilbert-Schmidt Independence Criterion takes way too long for even 10-20 variables. The big caveat is that partial correlation assumes the data is described by a linear Gaussian model, which obviously isn't true for scRNA-seq. It's a long-term goal to add different types of nonparametric conditional independence/invariance tests that can be run in a reasonable amount of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting computations on 50 cores\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-02 15:14:29.425284: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-02 15:14:29.427611: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-02 15:14:29.472118: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-02 15:14:30.274913: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-07-02 15:14:31.477946: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-02 15:14:31.480282: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-02 15:14:31.526199: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-02 15:14:32.122736: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-02 15:14:32.124888: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-02 15:14:32.170070: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-02 15:14:32.305544: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-07-02 15:14:32.558630: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-02 15:14:32.560987: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-02 15:14:32.607679: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-02 15:14:32.894828: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-02 15:14:32.897278: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-02 15:14:32.944184: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-02 15:14:32.966292: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-07-02 15:14:33.153065: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-02 15:14:33.156109: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-02 15:14:33.219625: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-02 15:14:33.398166: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-07-02 15:14:33.726080: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-07-02 15:14:34.234948: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-07-02 15:14:34.458834: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-02 15:14:34.461296: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-02 15:14:34.509577: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-02 15:14:34.905681: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-02 15:14:34.907948: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-02 15:14:34.952896: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-02 15:14:35.373941: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-07-02 15:14:35.442942: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-02 15:14:35.445188: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-02 15:14:35.491511: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-02 15:14:35.810517: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-07-02 15:14:36.324846: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-07-02 15:14:37.137253: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-02 15:14:37.139808: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-02 15:14:37.190799: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-02 15:14:37.839898: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-02 15:14:37.842380: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-02 15:14:37.887049: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-02 15:14:37.889440: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-02 15:14:37.890398: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-02 15:14:37.937931: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-02 15:14:38.109315: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-07-02 15:14:38.159474: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-02 15:14:38.162177: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-02 15:14:38.211281: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-02 15:14:38.819429: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-07-02 15:14:38.831900: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-07-02 15:14:38.926654: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-02 15:14:38.929383: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-02 15:14:38.983876: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-02 15:14:39.094843: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-07-02 15:14:39.795932: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-02 15:14:39.798444: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-02 15:14:39.843839: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-02 15:14:39.846321: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-02 15:14:39.848217: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-02 15:14:39.896143: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-02 15:14:39.999824: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-07-02 15:14:40.805595: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-07-02 15:14:40.822913: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-07-02 15:14:40.894970: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-02 15:14:40.897840: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-02 15:14:40.953819: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-02 15:14:41.066100: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-02 15:14:41.068683: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-02 15:14:41.120533: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-02 15:14:41.344561: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-02 15:14:41.347134: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-02 15:14:41.399396: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-02 15:14:41.848992: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-02 15:14:41.851927: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-02 15:14:41.908163: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-02 15:14:41.986201: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-07-02 15:14:42.128122: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-07-02 15:14:42.414168: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-07-02 15:14:42.971155: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-07-02 15:14:43.082499: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-02 15:14:43.085123: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-02 15:14:43.104916: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-02 15:14:43.107409: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-02 15:14:43.138082: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-02 15:14:43.159680: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-02 15:14:43.722526: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-02 15:14:43.725466: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-02 15:14:43.783332: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-02 15:14:44.168242: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-02 15:14:44.171156: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-02 15:14:44.201118: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-07-02 15:14:44.206436: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-07-02 15:14:44.230206: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-02 15:14:44.895057: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-07-02 15:14:45.339418: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-02 15:14:45.341762: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-07-02 15:14:45.342152: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-02 15:14:45.396122: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-02 15:14:45.509088: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-02 15:14:45.512188: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-02 15:14:45.572879: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-02 15:14:46.004468: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-02 15:14:46.007458: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-02 15:14:46.066871: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-02 15:14:46.513546: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-07-02 15:14:46.755480: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-07-02 15:14:47.234165: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-07-02 15:14:47.457671: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-02 15:14:47.460791: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-02 15:14:47.521057: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-02 15:14:47.817457: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-02 15:14:47.820630: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-02 15:14:47.881807: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-02 15:14:48.708711: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-07-02 15:14:48.932879: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-02 15:14:48.935862: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-02 15:14:48.997566: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-02 15:14:49.068366: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-07-02 15:14:49.538789: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-02 15:14:49.542186: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-02 15:14:49.605396: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-02 15:14:49.698959: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-02 15:14:49.701916: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-02 15:14:49.763076: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-02 15:14:50.720577: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-02 15:14:50.723992: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-02 15:14:50.787742: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-02 15:14:50.843558: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-07-02 15:14:50.877225: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-07-02 15:14:50.972360: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-07-02 15:14:51.256363: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-02 15:14:51.256363: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-02 15:14:51.259617: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-02 15:14:51.259629: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-02 15:14:51.319806: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-02 15:14:51.320747: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-02 15:14:52.056117: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-07-02 15:14:52.580152: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-07-02 15:14:52.643471: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-07-02 15:14:52.883725: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-02 15:14:52.886637: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-02 15:14:52.945724: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-02 15:14:53.611235: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-02 15:14:53.614597: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-02 15:14:53.681929: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-02 15:14:54.234581: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-07-02 15:14:54.627797: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-02 15:14:54.631404: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-02 15:14:54.691963: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-02 15:14:54.694866: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-02 15:14:54.699552: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-02 15:14:54.754005: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-02 15:14:55.007642: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-07-02 15:14:55.519506: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-02 15:14:55.522831: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-02 15:14:55.583281: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-02 15:14:55.996334: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-07-02 15:14:56.115880: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-07-02 15:14:56.208412: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-02 15:14:56.212187: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-02 15:14:56.279033: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-02 15:14:57.288651: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-07-02 15:14:57.666519: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-07-02 15:15:00.790747: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-02 15:15:00.794780: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-02 15:15:00.865221: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-02 15:15:00.875791: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-02 15:15:00.880145: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-02 15:15:00.972782: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-02 15:15:01.216747: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-02 15:15:01.222959: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-02 15:15:01.337910: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-02 15:15:02.889651: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-07-02 15:15:03.129235: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-07-02 15:15:03.702837: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-07-02 15:15:06.351028: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-02 15:15:06.355030: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-02 15:15:06.428158: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-02 15:15:07.282144: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-02 15:15:07.288246: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-02 15:15:07.406664: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-02 15:15:08.061486: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-07-02 15:15:09.582901: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-02 15:15:09.587961: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-02 15:15:09.698436: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-02 15:15:09.720818: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-07-02 15:15:11.998657: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-07-02 15:15:13.543162: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-02 15:15:13.550483: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-02 15:15:13.653931: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-02 15:15:13.659369: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-02 15:15:13.676236: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-02 15:15:13.788412: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-02 15:15:15.180570: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-02 15:15:15.185942: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-02 15:15:15.297458: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-02 15:15:15.911709: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-07-02 15:15:16.176032: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-07-02 15:15:17.680992: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time: 796.1604704409838\n"
     ]
    }
   ],
   "source": [
    "fs.tl.learn_intercellular_flows(adata,\n",
    "                        condition_key = condition_key,\n",
    "                        control_key = 'MASL',  #CTRL, MASL\n",
    "                        flowsig_key = 'flowsig_network',\n",
    "                        flow_expr_key = 'X_flow',\n",
    "                        use_spatial = False,\n",
    "                        n_jobs = 50,\n",
    "                        n_bootstraps = 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partially validate intercellular flow network\n",
    "Finally, we will remove any \"false positive\" edges. Noting that the CPDAG contains directed arcs and undirected arcs we do two things.\n",
    "\n",
    "First, we remove directed arcs that are not oriented from signal inflow to GEM, GEM to GEM, or from GEM to signal outflow and for undirected edges, we reorient them so that they obey the previous directionalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.tl.apply_biological_flow(adata,\n",
    "                            flowsig_network_key = 'flowsig_network',\n",
    "                            adjacency_key = 'adjacency',\n",
    "                            validated_key = 'validated')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we will remove directed arcs whose bootstrapped frequencies are below a specified edge threshold as well as undirected edges whose total bootstrapped frequencies are below the same threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.    0.    0.    ... 0.    0.    0.   ]\n",
      " [0.    0.    0.    ... 0.514 0.526 0.502]\n",
      " [0.    0.    0.    ... 0.    0.064 0.066]\n",
      " ...\n",
      " [0.    0.476 0.    ... 0.    0.    0.42 ]\n",
      " [0.    0.496 0.928 ... 0.    0.    0.   ]\n",
      " [0.    0.542 0.934 ... 0.496 0.    0.   ]]\n"
     ]
    }
   ],
   "source": [
    "#Every time we apply these steps, we generate a new adjacency matrix that describes the intercellular flow network edges. \n",
    "# The original output from learn_intercellular_flows is stored in adata.uns['flowsig_network]['network']['adjacency']. \n",
    "# After validation using apply_biological_flow, we add the _validated key to the adjacency key, so that the new adjacency is stored in adata.uns['flowsig_network]['network']['adjacency_validated']. \n",
    "# After filtering low-confidence edges, the adjacency is stored under adata.uns['flowsig_network]['network']['adjacency_validated_filtered']. \n",
    "# As a note, you could change the order of these two post-processing steps, which would mean that the final adjacency would be under the key adjacency_filtered_validated. The results should be similar but it's worth remembering this.\n",
    "edge_threshold = 0.8\n",
    "fs.tl.filter_low_confidence_edges(adata,\n",
    "                                edge_threshold = edge_threshold,\n",
    "                                flowsig_network_key = 'flowsig_network',\n",
    "                                adjacency_key = 'adjacency_validated',\n",
    "                                filtered_key = 'filtered')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and reload the flowsig outputs\n",
    "#adata.write('/data/project/AI4Omic/MASLD/results/scRNA/flowsig/adata_MASL_flowsig.h5ad')\n",
    "#adata = sc.read_h5ad('/data/project/AI4Omic/MASLD/results/scRNA/flowsig/adata_MASL_flowsig.h5ad')\n",
    "#adata.write('/data/project/AI4Omic/MASLD/results/scRNA/flowsig/adata_MASH_flowsig.h5ad')\n",
    "adata = sc.read_h5ad('/data/project/AI4Omic/MASLD/results/scRNA/flowsig/adata_MASH_flowsig.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GEMs explained by celltypes and conditions (disease status)\n",
    "marker = [f'GEM-{i}' for i in range(1, adata.obsm['X_gem'].shape[1]+1)]\n",
    "gem_df = pd.DataFrame(adata.obsm['X_gem'], columns=marker)\n",
    "\n",
    "gem_df.index = adata.obs.index\n",
    "adata.obs = adata.obs.join(gem_df)\n",
    "\n",
    "sc.pl.dotplot(adata, var_names=marker, groupby='disease_status', save=\"GEM_Condition_MASH_versus_MASL.pdf\")\n",
    "#sc.pl.dotplot(adata, var_names=marker, groupby='disease_status', save=\"GEM_Condition_MASL_versus_CTRL.pdf\")\n",
    "#sc.pl.heatmap(adata, var_names=marker, groupby='disease_status', cmap='Reds', vmax=0.10, vmin=0, save=\"Condition.pdf\")\n",
    "\n",
    "sc.pl.dotplot(adata, var_names=marker, groupby='cell_type_lvl2', save=\"GEM_celltype_MASH_versus_MASL.pdf\", dendrogram=True)\n",
    "#sc.pl.dotplot(adata, var_names=marker, groupby='cell_type_lvl2', save=\"GEM_celltype_MASL_versus_CTRL.pdf\", dendrogram=True)\n",
    "#sc.pl.heatmap(adata, var_names=marker, groupby='cell_type_lvl2', cmap='Reds', vmax=0.10, vmin=0, save=\"celltype.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also note that if you want to explore the network directly, we included a function to generate the directed NetworkX DiGraph object. You will need to generate this for any of the plotting functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_network = fs.tl.construct_intercellular_flow_network(adata, flowsig_network_key = 'flowsig_network', adjacency_key = 'adjacency_validated_filtered')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CALCRL', 'CSF1R', 'IL13RA2', 'IL1R2', 'IL4R+IL13RA1', 'LIFR+IL6ST', 'LRP1', 'NRP2+PLXNA1']\n",
      "Filtered inflow_vars: ['CALCRL', 'CSF1R', 'IL13RA2', 'IL1R2', 'IL4R+IL13RA1', 'LIFR+IL6ST', 'LRP1', 'NRP2+PLXNA1']\n"
     ]
    }
   ],
   "source": [
    "flow_var_info = adata.uns['flowsig_network']['flow_var_info']\n",
    "inflow_vars = flow_var_info.index[flow_var_info['Type'] == 'inflow'].tolist()\n",
    "print(inflow_vars)\n",
    "inflow_vars_filtered = [node for node in inflow_vars if node in flow_network.nodes]\n",
    "print(\"Filtered inflow_vars:\", inflow_vars_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['IL1R2',\n",
       " 'IL4R+IL13RA1',\n",
       " 'IL13RA2',\n",
       " 'LIFR+IL6ST',\n",
       " 'NRP2+PLXNA1',\n",
       " 'CSF1R',\n",
       " 'LRP1',\n",
       " 'CALCRL']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flow_vars = list(flow_network.nodes())\n",
    "flow_var_info = adata.uns['flowsig_network']['flow_var_info']\n",
    "[node for node in flow_vars if flow_var_info.loc[node]['Type'] == 'outflow']\n",
    "[node for node in flow_vars if flow_var_info.loc[node]['Type'] == 'inflow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predecessors of HGF:\n",
      "['GEM-30', 'GEM-20', 'GEM-24', 'GEM-28', 'GEM-10', 'GEM-25', 'GEM-23', 'GEM-13', 'GEM-5', 'GEM-29']\n",
      "Predecessors of IL1A:\n",
      "['GEM-14', 'GEM-12', 'GEM-18', 'GEM-23', 'GEM-5', 'GEM-29', 'GEM-17', 'GEM-13', 'GEM-8', 'GEM-30']\n",
      "Predecessors of CXCL9:\n",
      "['GEM-5', 'GEM-30', 'GEM-13', 'GEM-28', 'GEM-29', 'GEM-20', 'GEM-16', 'GEM-11']\n",
      "Predecessors of WNT10A:\n",
      "['GEM-10', 'GEM-17']\n",
      "Predecessors of CXCL3:\n",
      "['GEM-10', 'GEM-17', 'GEM-16', 'GEM-21', 'GEM-24']\n",
      "Successors of LRP1:\n",
      "['GEM-14', 'GEM-8', 'GEM-5', 'GEM-13', 'GEM-28', 'GEM-29', 'GEM-30', 'GEM-20', 'GEM-12']\n",
      "Successors of NRP2+PLXNA1:\n",
      "['GEM-29', 'GEM-12', 'GEM-14', 'GEM-21', 'GEM-16', 'GEM-8', 'GEM-24']\n",
      "Predecessors of GEM-13:\n",
      "['GEM-17', 'GEM-3', 'GEM-5', 'GEM-15', 'GEM-27', 'GEM-2', 'GEM-14', 'GEM-26', 'GEM-30', 'CALCRL', 'GEM-21', 'GEM-20', 'GEM-11', 'GEM-23', 'LRP1', 'LIFR+IL6ST', 'GEM-10', 'GEM-6', 'GEM-28', 'GEM-29', 'GEM-18', 'GEM-8']\n",
      "Successors of GEM-13:\n",
      "['GEM-17', 'GEM-3', 'GEM-5', 'GEM-15', 'CXCL9', 'GEM-27', 'GEM-2', 'GEM-14', 'GEM-26', 'GEM-30', 'TNFSF15', 'ANGPTL1', 'GEM-21', 'CXCL1', 'SPP1', 'GEM-20', 'GEM-11', 'IL1A', 'GEM-23', 'GEM-10', 'GEM-6', 'HGF', 'GEM-28', 'GEM-29', 'GEM-18', 'GEM-8']\n",
      "Predecessors of GEM-17:\n",
      "['GEM-29', 'GEM-13', 'GEM-1', 'GEM-4', 'CALCRL', 'GEM-8', 'GEM-10', 'GEM-3', 'GEM-5', 'GEM-24', 'GEM-28', 'GEM-21', 'GEM-26', 'GEM-30', 'GEM-2', 'GEM-27', 'GEM-25', 'GEM-23', 'GEM-20']\n",
      "Successors of GEM-17:\n",
      "['GEM-29', 'GEM-13', 'GEM-1', 'GEM-4', 'GEM-8', 'GEM-10', 'CTSG', 'CXCL6', 'IL1A', 'GEM-3', 'GEM-5', 'GEM-24', 'CXCL3', 'GEM-28', 'GEM-21', 'GEM-26', 'GEM-30', 'GEM-2', 'GEM-27', 'WNT10A', 'GEM-25', 'GEM-23', 'GEM-20']\n",
      "Predecessors of GEM-10:\n",
      "['GEM-12', 'GEM-14', 'GEM-17', 'GEM-5', 'GEM-1', 'GEM-27', 'CALCRL', 'GEM-13', 'GEM-26', 'GEM-23', 'GEM-25', 'GEM-28', 'GEM-20', 'GEM-18', 'GEM-30', 'GEM-2', 'GEM-15', 'GEM-6', 'GEM-4', 'GEM-3']\n",
      "Successors of GEM-10:\n",
      "['WNT10A', 'GEM-12', 'GEM-14', 'GEM-17', 'GEM-5', 'GEM-1', 'CXCL3', 'GEM-27', 'HGF', 'GEM-13', 'GEM-26', 'GEM-23', 'GEM-25', 'CXCL6', 'GEM-28', 'GEM-20', 'GEM-18', 'CTSG', 'GEM-30', 'CXCL1', 'GEM-2', 'GEM-15', 'GEM-6', 'GEM-4', 'GEM-3']\n"
     ]
    }
   ],
   "source": [
    "# 查看HGF的前继节点\n",
    "for outflow_var in ['HGF', 'IL1A', 'CXCL9', 'WNT10A', 'CXCL3']:\n",
    "    print(f\"Predecessors of {outflow_var}:\")\n",
    "    print(list(flow_network.predecessors(outflow_var)))\n",
    "for inflow_var in ['LRP1', 'NRP2+PLXNA1']:\n",
    "    print(f\"Successors of {inflow_var}:\")\n",
    "    print(list(flow_network.successors(inflow_var)))\n",
    "for GEM in ['GEM-13', 'GEM-17', 'GEM-10']:\n",
    "    print(f\"Predecessors of {GEM}:\")\n",
    "    print(list(flow_network.predecessors(GEM)))\n",
    "    print(f\"Successors of {GEM}:\")\n",
    "    print(list(flow_network.successors(GEM)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glocal intercellular flow plot for MASL versus CTRL\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "fs.pl.plot_intercellular_flows(adata,\n",
    "                                flow_network=flow_network,\n",
    "                                flowsig_network_key='flowsig_network',\n",
    "                                inflow_vars=['CXCR3', 'KDR', 'LIFR+IL6ST', 'IL13RA2', 'CALCRL'],\n",
    "                                outflow_vars=['MIF', 'IL33', 'ANGPTL1'],\n",
    "                                module_vars=['GEM-1', 'GEM-4', 'GEM-5', 'GEM-11', 'GEM-14', 'GEM-21', 'GEM-19', 'GEM-22', 'GEM-24'],\n",
    "                                align_mode='horizontal',\n",
    "                                width_scale=1.0,\n",
    "                                x_margin_offset=0.0,\n",
    "                                y_margin_offset=0.0,\n",
    "                                ax=ax\n",
    "                                )\n",
    "plt.tight_layout()\n",
    "plt.grid(False)\n",
    "plt.savefig('/data/project/AI4Omic/MASLD/results/scRNA/flowsig/flowsig_intercellular_flow_MASL_versus_CTRL.pdf', dpi=600, transparent=True, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n"
     ]
    }
   ],
   "source": [
    "# glocal intercellular flow plot for MASH versus MASL\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "fs.pl.plot_intercellular_flows(adata,\n",
    "                                flow_network=flow_network,\n",
    "                                flowsig_network_key='flowsig_network',\n",
    "                                inflow_vars=['LRP1', 'NRP2+PLXNA1', 'IL4R+IL13RA1'], # \n",
    "                                outflow_vars=['CXCL3', 'HGF', 'IL1A',  'WNT10A', 'CXCL9'],\n",
    "                                module_vars=['GEM-5', 'GEM-12', 'GEM-14', 'GEM-13', 'GEM-29', 'GEM-15', 'GEM-30', 'GEM-28', 'GEM-1', 'GEM-3'],\n",
    "                                align_mode='horizontal',\n",
    "                                width_scale=1.0,\n",
    "                                x_margin_offset=0.0,\n",
    "                                y_margin_offset=0.0,\n",
    "                                ax=ax\n",
    "                                )\n",
    "plt.tight_layout()\n",
    "plt.grid(False)\n",
    "plt.savefig('/data/project/AI4Omic/MASLD/results/scRNA/flowsig/flowsig_intercellular_flow_MASH_versus_MASL.pdf', dpi=600, transparent=True, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.dotplot(adata_ref, ['HGF', \"CXCL3\", 'WNT10A', 'IL1A', 'CXCL9', 'ANGPTL1', 'IL33', 'MIF'], groupby='cell_type_lvl2', save='_outflow_signals', swap_axes=True, cmap='Blues', vmax=1.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flowsig",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
